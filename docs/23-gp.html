<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-03-31 Tue 07:32 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Introduction to Gaussian Process Regression</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="John Kitchin" />
<meta name="keywords" content="Gaussian process" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Introduction to Gaussian Process Regression</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org7623e1d">1. Gaussian Process Regression</a>
<ul>
<li><a href="#org9421561">1.1. Regular regression - models with parameters</a></li>
<li><a href="#org6656f23">1.2. Machine learning regression - flexible models with parameters</a></li>
<li><a href="#org9cb6b6a">1.3. Interpolation schemes</a></li>
<li><a href="#org30a5a70">1.4. Gaussian process regression (GPR)</a></li>
</ul>
</li>
<li><a href="#orgeb825a3">2. GPR by example</a>
<ul>
<li><a href="#org4a31bba">2.1. Underfitting in GPR</a></li>
<li><a href="#org1732665">2.2. Overfitting in GPR</a></li>
<li><a href="#orgb611891">2.3. Finding the hyperparameters in GPR</a></li>
</ul>
</li>
<li><a href="#org2f9c039">3. GPR Kernels</a>
<ul>
<li><a href="#org0bfd170">3.1. An example with a linear kernel</a>
<ul>
<li><a href="#org8864285">3.1.1. Uncertainty quantification in GPR</a></li>
</ul>
</li>
<li><a href="#org51e0b20">3.2. Combining kernels</a></li>
</ul>
</li>
<li><a href="#orgb709d7e">4. Brief comparison of GPR and NN</a></li>
<li><a href="#orgb9d0b40">5. GPR libraries</a></li>
<li><a href="#orgb14a901">6. Summary</a></li>
</ul>
</div>
</div>

<div id="outline-container-org7623e1d" class="outline-2">
<h2 id="org7623e1d"><span class="section-number-2">1</span> Gaussian Process Regression</h2>
<div class="outline-text-2" id="text-1">
<p>
An alternative approach to data-driven models is Gaussian Process Regression. It is so different from the other kinds of regression we have done so far that we will need to take some time unraveling what it is and how to use it. First we briefly review what we have done so far.
</p>
</div>

<div id="outline-container-org9421561" class="outline-3">
<h3 id="org9421561"><span class="section-number-3">1.1</span> Regular regression - models with parameters</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Most of what we have done is use models (e.g. a line, polynomial, exponential, etc.) with parameters in them that we fit to some data. We often can interpret those parameters in meaningful ways, e.g. the slope of the line, or a rate constant, etc. We worked out a way to estimate the uncertainty of the parameters, and it is possible to propagate that uncertainty to predictions of the model.
</p>
</div>
</div>

<div id="outline-container-org6656f23" class="outline-3">
<h3 id="org6656f23"><span class="section-number-3">1.2</span> Machine learning regression - flexible models with parameters</h3>
<div class="outline-text-3" id="text-1-2">
<p>
We expanded our thinking of models, and developed a way to build very flexible models that could be nonlinear, and that had a lot of adjustable parameters. These parameters were still found by fitting, e.g. minimizing the summed squared errors, to data. We gave up the interpretability of the parameters in favor of the flexibility to fit the data.
</p>
</div>
</div>

<div id="outline-container-org9cb6b6a" class="outline-3">
<h3 id="org9cb6b6a"><span class="section-number-3">1.3</span> Interpolation schemes</h3>
<div class="outline-text-3" id="text-1-3">
<p>
We also considered a few interpolating schemes. In these schemes, you assume some functional form exists between data points, locally fit the data, and then use the local fit to make predictions about intermediate points. Typical functional forms are linear, quadratic or cubic splines.
</p>
</div>
</div>

<div id="outline-container-org30a5a70" class="outline-3">
<h3 id="org30a5a70"><span class="section-number-3">1.4</span> Gaussian process regression (GPR)</h3>
<div class="outline-text-3" id="text-1-4">
<p>
GPR is somewhat intermediate in these ideas: It is like an interpolation scheme in the sense that we will make estimates of new points as weighted sums of known points. The weights, however, will be computed by an assumed model that has some parameters in it that must be fitted. These parameters are usually not directly meaningful though.
</p>

<p>
There is a substantial history and mathematical foundation behind GPR. In this lecture we will take a very practical approach based on some definitions for GPR. This will have the benefit that at the end you will know what it is and how to do it, but not where the definitions come from. That is not necessary to understand what is done, but if you are interested, here are some resources for learning more.
</p>

<p>
Automatic Model Construction with Gaussian Processes - David Duvenaud PhD thesis
<a href="https://www.cs.toronto.edu/~duvenaud/thesis.pdf">https://www.cs.toronto.edu/~duvenaud/thesis.pdf</a>
</p>

<p>
Gaussian Processes for Machine Learning
<a href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf">http://www.gaussianprocess.org/gpml/chapters/RW.pdf</a>
</p>
</div>
</div>
</div>

<div id="outline-container-orgeb825a3" class="outline-2">
<h2 id="orgeb825a3"><span class="section-number-2">2</span> GPR by example</h2>
<div class="outline-text-2" id="text-2">
<p>
Let's start with a goal, which is given some set of data points \(x_i, y_i\) we would like to predict the value of \(y*\) at some new point \(x*\), and we would like that prediction to have the form of \(y* = \sum_i w_i y_i\). That is we want the predicted value to be a weighted sum of the known points.  The key challenge is how to compute those weights.
</p>

<p>
To motivate why this is a reasonable idea, recall the idea behind Gaussian quadrature for integrals (<a href="https://en.wikipedia.org/wiki/Gaussian_quadrature">https://en.wikipedia.org/wiki/Gaussian_quadrature</a>): we can compute the value of an integral as the weighted sum of a few special points. The integral is a kind of function, and surely if those special points were among our data points, we could just weight all the other points by 0 and achieve this goal.
</p>

<p>
Alternatively, consider how you might estimate it in your head. You would look at the data, and use points that are close to the data point you want to estimate to form the estimate. Points that are far from the data point would have less influence on your estimate. You are implicitly weighting the value of the known points in doing this. In GPR we make this idea quantitative.
</p>

<p>
The key concept to quantify this is called <i>covariance</i>, which is how are two variables correlated with each other. Intuitively, if two x-values are close together, then we anticipate that the corresponding \(f(x\) values are also close together. We can say that the values "co-vary", i.e. they are not independent. We use this fact when we integrate an ODE and estimate the next point, or in root solving when we iteratively find the next steps. We will use this idea to compute the weights that we need. The covariance is a matrix and each element of the matrix defines the covariance between two data points. To compute this, <i>we need to make some assumptions</i> about the data. A common assumption is that the covariance is Gaussian with the form:
</p>

<p>
\(K_{ij} = \sigma_f \exp\left(-\frac{(x_i - x_j)^2}{2 \lambda^2}\right)\)
</p>

<p>
In this equation, \(\sigma_f\) and \(\lambda\) are called <i>hyperparameters</i> and we have to determine what are good values for them. \(\sigma_f\) is a scale factor, and \(\lambda\) is a length scale. With this formula, data points with a distance of \(2\lambda\) away from the point of interest have low (near zero) covariance with that point. In other words, only data points within \(2\lambda\) distance units of a point will contribute to our estimate for that point.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">import</span> numpy <span style="color: #0000FF;">as</span> np

<span style="color: #BA36A5;">sigmaf</span> = 1.0
<span style="color: #BA36A5;">_lambda</span> = 1.0

<span style="color: #BA36A5;">x0</span> = 0.0
<span style="color: #BA36A5;">x</span> = np.linspace(-4, 4)

%matplotlib inline
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt
plt.plot(x, sigmaf * np.exp(-(x0 - x)**2 / (2 * _lambda**2)))
plt.xlabel(<span style="color: #008000;">'x'</span>)
plt.ylabel(<span style="color: #008000;">'covariance'</span>);
</pre>
</div>

<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>



<div class="figure">
<p><img src="obipy-resources/ff89c57eaf339be5985eacf89c18aa90a6f59ccd/aa827d432a6c1188468a45e95a1ee4398285c0b9.png" alt="aa827d432a6c1188468a45e95a1ee4398285c0b9.png" />
</p>
</div>

<p>
So, what we need is a convenient way to compute the covariance between the points we know, and the points we want to estimate. To keep things simple for now, we consider a small data set. We need to be able to compute the distance from each known \(x_i\) to each known \(x_j\). Numpy array broadcasting makes this simple. We <i>expand</i> each array, and then take the difference.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">X</span> = np.linspace(0, 1, 3)
<span style="color: #BA36A5;">y</span> = X**(1/3) + np.random.normal(0,  0.01, <span style="color: #006FE0;">len</span>(X)) <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">we add a little noise</span>

<span style="color: #BA36A5;">Xp</span> = np.array([0.45, 0.55])

<span style="color: #BA36A5;">dX</span> = X[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :]
dX
</pre>
</div>

<pre class="example">
array([[ 0. , -0.5, -1. ],
       [ 0.5,  0. , -0.5],
       [ 1. ,  0.5,  0. ]])
</pre>

<p>
First, we get the covariance array for <i>the known x-values</i>. We have to make some choices for the hyperparameters. We will return to how to do this later. For now, we use these values because they work. We also add at this point the possibility that there is some noise in our data, which is characterized by a normal distribution with a mean of 0, and a spread of \(\sigma_n\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">sigma_f</span> = 1
<span style="color: #BA36A5;">lam</span> = 0.15
<span style="color: #BA36A5;">sigma_n</span> = 0.01

<span style="color: #BA36A5;">K1</span> = sigma_f * np.exp(-dX**2 / (2 * lam**2)) + sigma_n**2 * np.eye(<span style="color: #006FE0;">len</span>(y))
K1
</pre>
</div>

<pre class="example">
array([[1.00010000e+00, 3.86592014e-03, 2.23363144e-10],
       [3.86592014e-03, 1.00010000e+00, 3.86592014e-03],
       [2.23363144e-10, 3.86592014e-03, 1.00010000e+00]])
</pre>

<p>
Next, we get the covariance of the <i>values of x we want to predict</i> and the known x-values. Note here we do not include the noise.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">K2</span> = sigma_f * np.exp(-(Xp[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])**2 / (2 * lam**2))
K2
</pre>
</div>

<pre class="example">
array([[0.011109  , 0.94595947, 0.00120386],
       [0.00120386, 0.94595947, 0.011109  ]])
</pre>


<p>
The first definition that we need is:
</p>

<p>
\(\mathbf{w} = K(X*, X) \cdot [K(X, X) + \sigma_n^2 \mathbf{I}]^{-1}\)
</p>

<p>
Here, \(\sigma_n\) is a constant that represents noise. It can be zero, but during fitting it is helpful for it to be non-zero to avoid ill-conditioned matrices.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">w</span> = K2 @ np.linalg.inv(K1)
w
</pre>
</div>

<pre class="example">
array([[ 0.00745169,  0.94584556, -0.00245246],
       [-0.00245246,  0.94584556,  0.00745169]])
</pre>

<p>
Those weights mean that the middle data point contributes the most to the estimate, and the others hardly contribute.
</p>

<p>
To make an estimate with these weights, we use this second definition:
</p>

<p>
\(y* = \mathbf{w} \cdot \mathbf{y}\)
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">yp</span> = w @ y
yp
</pre>
</div>

<pre class="example">
array([0.74432431, 0.75433312])
</pre>

<p>
Let's see how well we did.
</p>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt

plt.plot(X, y, <span style="color: #008000;">'bo'</span>)
plt.plot(Xp, yp, <span style="color: #008000;">'r*'</span>)
</pre>
</div>

<pre class="example">
[&lt;matplotlib.lines.Line2D at 0x1109091d0&gt;]
</pre>


<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>



<div class="figure">
<p><img src="obipy-resources/ff89c57eaf339be5985eacf89c18aa90a6f59ccd/63dcbf3e21ea53af807f2d654e118d28f73a8cd6.png" alt="63dcbf3e21ea53af807f2d654e118d28f73a8cd6.png" />
</p>
</div>

<p>
That is not bad, but clearly not great. With a \(\lambda=0.15\), only one data point is contributing to the estimate, the other points have only small contributions because they are far from points we are estimating. This is a feature of the <i>assumption</i> we made about the covariance with \(\lambda\). This means we do not have enough data to make a very good estimate. We can see this if we try this with a much more dense data set.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">X</span>, <span style="color: #BA36A5;">h</span> = np.linspace(0, 1, 20, retstep=<span style="color: #D0372D;">True</span>)
<span style="color: #BA36A5;">y</span> = X**(1/3) + np.random.normal(0, 0.01, <span style="color: #006FE0;">len</span>(X))

<span style="color: #BA36A5;">xp</span> = np.linspace(0, 1.0, 50)

<span style="color: #BA36A5;">sigma_n</span> = 0.01
<span style="color: #BA36A5;">sigma_f</span> = 1
<span style="color: #BA36A5;">lam</span> = 0.15

<span style="color: #BA36A5;">K1</span> = sigma_f * np.exp(-(X[<span style="color: #D0372D;">None</span>, :] - X[:, <span style="color: #D0372D;">None</span>])**2 / (2 * lam**2)) + sigma_n**2 * np.eye(<span style="color: #006FE0;">len</span>(y))
<span style="color: #BA36A5;">Kp</span> = sigma_f * np.exp(-(X[<span style="color: #D0372D;">None</span>, :] - xp[:, <span style="color: #D0372D;">None</span>])**2 / (2 * lam**2))

<span style="color: #BA36A5;">yp</span> = Kp @ np.linalg.inv(K1) @ y

%matplotlib inline
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt

plt.plot(X, y, <span style="color: #008000;">'bo'</span>)
plt.plot(xp, yp, <span style="color: #008000;">'r-'</span>)
plt.legend([<span style="color: #008000;">'data'</span>, <span style="color: #008000;">'fit'</span>])
<span style="color: #0000FF;">print</span>(f<span style="color: #008000;">'Known data step size is </span><span style="color: #BA36A5;">{h:1.2f}</span><span style="color: #008000;">'</span>)
</pre>
</div>

<p>
Known data step size is 0.05
</p>

<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>



<div class="figure">
<p><img src="obipy-resources/ff89c57eaf339be5985eacf89c18aa90a6f59ccd/6ffb86c1e3e6fb7fb51d5b0e807dd02fd7c70d4d.png" alt="6ffb86c1e3e6fb7fb51d5b0e807dd02fd7c70d4d.png" />
</p>
</div>

<p>
Now you can see that we do very well in estimating the values. The length scale here might even be considered too short, since it is evident we are fitting trends in the noise.
</p>

<p>
GPR is often called a kind of machine learning. Let's see if the GPR actually "learned" the data by testing it in extrapolation.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">xp</span> = np.linspace(-1, 2.0, 50)

<span style="color: #BA36A5;">sigma_f</span> = 1
<span style="color: #BA36A5;">lam</span> = 0.15

<span style="color: #BA36A5;">K1</span> = sigma_f * np.exp(-(X[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])**2 / (2 * lam**2)) + sigma_n**2 * np.eye(<span style="color: #006FE0;">len</span>(y))
<span style="color: #BA36A5;">Kp</span> = sigma_f * np.exp(-(xp[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])**2 / (2 * lam**2))

<span style="color: #BA36A5;">yp</span> = Kp @ np.linalg.inv(K1) @ y

%matplotlib inline
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt

plt.plot(X, y, <span style="color: #008000;">'bo'</span>)
plt.plot(xp, yp, <span style="color: #008000;">'r-'</span>)
plt.legend([<span style="color: #008000;">'data'</span>, <span style="color: #008000;">'fit'</span>])
<span style="color: #0000FF;">print</span>(f<span style="color: #008000;">'Known data step size is </span><span style="color: #BA36A5;">{h:1.2f}</span><span style="color: #008000;">'</span>)
</pre>
</div>

<p>
Known data step size is 0.05
</p>

<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>



<div class="figure">
<p><img src="obipy-resources/ff89c57eaf339be5985eacf89c18aa90a6f59ccd/1efd65967d8478bda932c947547c81762cb4c275.png" alt="1efd65967d8478bda932c947547c81762cb4c275.png" />
</p>
</div>

<p>
As we saw with neural networks, GPRs do not extrapolate in a way that reflects the data. Eventually, in this case the result extrapolates to zero because of the Gaussian covariance function, but there are edge effects that are not desirable. As with Nns, we should be wary of extrapolation. We return to this in a later section.
</p>
</div>

<div id="outline-container-org4a31bba" class="outline-3">
<h3 id="org4a31bba"><span class="section-number-3">2.1</span> Underfitting in GPR</h3>
<div class="outline-text-3" id="text-2-1">
<p>
If you make the lengthscale too large then you over smooth the data, and don't fit any of them on average. This is underfitting, and it is not desirable because the estimates will not be good at new points. Note that you need some noise in the covariance array to make sure it is invertible in this case.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">xp</span> = np.linspace(0, 1.0, 50)

<span style="color: #BA36A5;">sigma_f</span>, <span style="color: #BA36A5;">lam</span>, <span style="color: #BA36A5;">sigma_n</span> = 1, 3, 0.01

<span style="color: #BA36A5;">K1</span> = sigma_f * np.exp(-(X[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])**2 / (2 * lam**2)) + sigma_n**2 * np.eye(<span style="color: #006FE0;">len</span>(y))
<span style="color: #BA36A5;">Kp</span> = sigma_f * np.exp(-(xp[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])**2 / (2 * lam**2))

<span style="color: #BA36A5;">yp</span> = Kp @ np.linalg.inv(K1) @ y

%matplotlib inline
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt

plt.plot(X, y, <span style="color: #008000;">'bo'</span>)
plt.plot(xp, yp, <span style="color: #008000;">'r-'</span>)
plt.legend([<span style="color: #008000;">'data'</span>, <span style="color: #008000;">'fit'</span>])

</pre>
</div>

<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>



<div class="figure">
<p><img src="obipy-resources/ff89c57eaf339be5985eacf89c18aa90a6f59ccd/4c452e754ab4799e91563ead866b6fc33c91d3d0.png" alt="4c452e754ab4799e91563ead866b6fc33c91d3d0.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org1732665" class="outline-3">
<h3 id="org1732665"><span class="section-number-3">2.2</span> Overfitting in GPR</h3>
<div class="outline-text-3" id="text-2-2">
<p>
If you make the lengthscale too small, then you effectively fit every point, and have wiggles between them. This is overfitting, and it is not desirable because you won't get a good estimate at new points.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">sigma_f</span>, <span style="color: #BA36A5;">lam</span>, <span style="color: #BA36A5;">sigma_n</span> = 1, 0.05, 0.01

<span style="color: #BA36A5;">K1</span> = sigma_f * np.exp(-(X[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])**2 / (2 * lam**2)) + sigma_n**2 * np.eye(<span style="color: #006FE0;">len</span>(y))
<span style="color: #BA36A5;">Kp</span> = sigma_f * np.exp(-(xp[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])**2 / (2 * lam**2))

<span style="color: #BA36A5;">yp</span> = Kp @ np.linalg.inv(K1) @ y

%matplotlib inline
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt

plt.plot(X, y, <span style="color: #008000;">'bo'</span>)
plt.plot(xp, yp, <span style="color: #008000;">'r-'</span>)
plt.legend([<span style="color: #008000;">'data'</span>, <span style="color: #008000;">'fit'</span>])

</pre>
</div>

<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>



<div class="figure">
<p><img src="obipy-resources/ff89c57eaf339be5985eacf89c18aa90a6f59ccd/d76e2a4b04b28dbd3f3e5162b7dd02bbd3445585.png" alt="d76e2a4b04b28dbd3f3e5162b7dd02bbd3445585.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgb611891" class="outline-3">
<h3 id="orgb611891"><span class="section-number-3">2.3</span> Finding the hyperparameters in GPR</h3>
<div class="outline-text-3" id="text-2-3">
<p>
You can see from the examples above that we have to choose some compromises in the hyperparameters. Some sets will underfit, and some will overfit. So, we need some principled way to estimate these. In conventional regression we would do this by minimizing an error function. In GPR, we use a different approach called <i>maximizing the log likelihood</i> of the parameters. This is a statistical concept, that is similar to minimizing the summed squared error, but different in that it is estimating the most likely average value of the hyperparameters. It is also must an optimization problem, that we formulate as:
</p>

<p>
\(logp \approx -0.5 y K^{-1} y - 0.5 \log |K|\)
</p>

<p>
The first term emphasizes fitting to the data, while the second term penalizes complexity. In this equation, \(K\) depends on the hyperparameters, and we want to adjust these to maximize \(logp\). Since we know something about the noise here, we fix that parameter, and adjust the other two parameters.
</p>

<p>
Given the original data, we now estimate the best hyperparameters and then predict other values.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">def</span> <span style="color: #006699;">log_likelihood</span>(params):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">sigmaf</span>, <span style="color: #BA36A5;">lam</span> = params
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">sigma_n</span> = 0.01
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">K</span> = sigma_f * np.exp(-(X[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])**2 / (2 * lam**2)) + sigma_n**2 * np.eye(<span style="color: #006FE0;">len</span>(y))
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> -1*(-0.5 * y @ np.linalg.inv(K) @ y - 0.5 * np.log(np.diag(K)).<span style="color: #006FE0;">sum</span>())

<span style="color: #BA36A5;">xp</span> = np.linspace(-1, 2.0, 50)

<span style="color: #BA36A5;">p</span> = minimize(log_likelihood, [1, 0.05])
<span style="color: #BA36A5;">sigma_f</span>, <span style="color: #BA36A5;">lam</span>, <span style="color: #BA36A5;">sigma_n</span> = (*p.x, 0.001)

<span style="color: #BA36A5;">K1</span> = sigma_f * np.exp(-(X[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])**2 / (2 * lam**2)) + sigma_n**2 * np.eye(<span style="color: #006FE0;">len</span>(y))
<span style="color: #BA36A5;">Kp</span> = sigma_f * np.exp(-(xp[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])**2 / (2 * lam**2))

<span style="color: #BA36A5;">yp</span> = Kp @ np.linalg.inv(K1) @ y

%matplotlib inline
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt

plt.plot(X, y, <span style="color: #008000;">'bo'</span>)
plt.plot(xp, yp, <span style="color: #008000;">'r-'</span>)
plt.legend([<span style="color: #008000;">'data'</span>, <span style="color: #008000;">'fit'</span>])

p.x
</pre>
</div>

<p>
NameErrorTraceback (most recent call last)
&lt;ipython-input-12-2c3c9cf749b8&gt; in &lt;module&gt;
      7 xp = np.linspace(-1, 2.0, 50)
      8
-&#x2014;&gt; 9 p = minimize(log_likelihood, [1, 0.05])
     10 sigma_f, lam, sigma_n = (*p.x, 0.001)
     11
</p>

<p>
NameError: name 'minimize' is not defined
</p>

<p>
Note that we still see some wiggles in the fit, indicating some minor degree of overfitting with the optimal hyperparameters. That is happening because we fit to all the data, and do not use any to estimate how good our fits are. You can use train/test data splits for GPR for this purpose as well, but it is out of the scope of the lecture today.
</p>

<p>
Also, note that the GPR doesn't <i>learn</i> the underlying function; it simply provides a weighted interpolation based on the covariance (assumed to be Gaussian) of neighboring points. The quality of the estimates depends on 1) the density of nearby points, and 2) whether Gaussian covariance is reasonable. When you have a lot of data that is close together, you can always get away with Gaussian covariance, but with small data sets of sparse points, it can be difficult to figure out reasonable hyperparameters. Also, Gaussian covariance does not extrapolate the way the underlying function here extrapolates.
</p>
</div>
</div>
</div>

<div id="outline-container-org2f9c039" class="outline-2">
<h2 id="org2f9c039"><span class="section-number-2">3</span> GPR Kernels</h2>
<div class="outline-text-2" id="text-3">
<p>
The function we used to compute the covariance arrays is called a <i>kernel</i>. It is in a way, a measure of similarity between two points. In the Gaussian kernel, we assume the similarity decays exponentially with the square of the distance between points, so that points that are more than a few lengthscales away are uncorrelated and have no information to contribute.
</p>

<p>
There are many other kinds of kernels, including linear and periodic kernels.
</p>
<ul class="org-ul">
<li><a href="https://peterroelants.github.io/posts/gaussian-process-kernels/">https://peterroelants.github.io/posts/gaussian-process-kernels/</a></li>
<li><a href="https://www.cs.toronto.edu/~duvenaud/cookbook/">https://www.cs.toronto.edu/~duvenaud/cookbook/</a></li>
</ul>

<p>
These kernels can be combined by multiplication and addition to form new kernels, allowing you to build sophisticated models for interpolating data.
</p>

<p>
Choosing a reasonable kernel is important, because it determines how well the model fits, and its extrapolation behavior (much like the activation functions in a NN).
</p>
</div>

<div id="outline-container-org0bfd170" class="outline-3">
<h3 id="org0bfd170"><span class="section-number-3">3.1</span> An example with a linear kernel</h3>
<div class="outline-text-3" id="text-3-1">
<p>
One definition of a linear kernel is
</p>

<p>
\(k(x, x*) = \sigma_b^2 + \sigma_v^2 (x-c)(x_{*}-c)\).
</p>

<p>
There are three hyperparameters in this kernel, \(\sigma_b, \sigma_v\) and \(c\). None of these are easily interpreted as properties of the line though. Instead, they represent properties of a distribution of lines that fit the data. We do not care about this distribution directly, but rather about their mean value which is what we are predicting.
</p>

<p>
We will use this to fit some linear data in this example.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">X</span> = np.linspace(0, 1, 10)
<span style="color: #BA36A5;">y</span> = 2 * X + 3 + np.random.normal(0, 0.05, <span style="color: #006FE0;">len</span>(X))

plt.plot(X, y, <span style="color: #008000;">'b.'</span>)
plt.xlabel(<span style="color: #008000;">'x'</span>)
plt.ylabel(<span style="color: #008000;">'y'</span>)
</pre>
</div>

<pre class="example">
Text(0, 0.5, 'y')
</pre>


<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>



<div class="figure">
<p><img src="obipy-resources/ff89c57eaf339be5985eacf89c18aa90a6f59ccd/bdec780bb3a86925594f9f03773ea2c0b065ea08.png" alt="bdec780bb3a86925594f9f03773ea2c0b065ea08.png" />
</p>
</div>

<p>
As before, we setup a log likelihood function and maximize it to get estimates for the parameters.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">def</span> <span style="color: #006699;">LL</span>(params):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">sb</span>, <span style="color: #BA36A5;">sv</span>, <span style="color: #BA36A5;">c</span> = params
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">K</span> = sb**2 + sv**2 * (X - c)[:, <span style="color: #D0372D;">None</span>] * (X - c)[<span style="color: #D0372D;">None</span>, :]
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">K</span> += 0.05 * np.eye(<span style="color: #006FE0;">len</span>(y))
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> -1*(-0.5 * y @ np.linalg.inv(K) @ y - 0.5 * np.log(np.diag(K)).<span style="color: #006FE0;">sum</span>())

<span style="color: #0000FF;">from</span> scipy.optimize <span style="color: #0000FF;">import</span> minimize
<span style="color: #BA36A5;">p</span> = minimize(LL, [3, 2, 0])
p
</pre>
</div>

<pre class="example">
     fun: 7.588856548455368
hess_inv: array([[ 5.55750179e+00, -1.27571635e-02, -4.24234266e-03],
      [-1.27571635e-02,  2.54351448e-02,  1.55291043e-02],
      [-4.24234266e-03,  1.55291043e-02,  4.88240506e-02]])
     jac: array([-2.20537186e-06,  5.00679016e-06,  2.74181366e-06])
 message: 'Optimization terminated successfully.'
    nfev: 185
     nit: 28
    njev: 37
  status: 0
 success: True
       x: array([-4.70916410e-06,  6.50151314e-01, -1.46061798e+00])
</pre>

<p>
And we can plot the function to see how well it does.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">sb</span>, <span style="color: #BA36A5;">sv</span>, <span style="color: #BA36A5;">c</span> = p.x

<span style="color: #BA36A5;">Xp</span> = np.linspace(-1, 2)

<span style="color: #BA36A5;">K</span> = sb**2 + sv**2 * (X - c)[:, <span style="color: #D0372D;">None</span>] * (X - c)[<span style="color: #D0372D;">None</span>, :]
<span style="color: #BA36A5;">K</span> += 0.05 * np.eye(<span style="color: #006FE0;">len</span>(y))

<span style="color: #BA36A5;">Kp</span> = sb**2 + sv**2 * (Xp - c)[:, <span style="color: #D0372D;">None</span>] * (X - c)[<span style="color: #D0372D;">None</span>, :]

<span style="color: #BA36A5;">yp</span> = Kp @ np.linalg.inv(K) @ y

plt.plot(X, y, <span style="color: #008000;">'b.'</span>)
plt.plot(Xp, yp)
plt.xlabel(<span style="color: #008000;">'x'</span>)
plt.ylabel(<span style="color: #008000;">'y'</span>)
plt.legend([<span style="color: #008000;">'data'</span>, <span style="color: #008000;">'GPR'</span>])
</pre>
</div>

<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>



<div class="figure">
<p><img src="obipy-resources/ff89c57eaf339be5985eacf89c18aa90a6f59ccd/ca549e6341642bdc7c9456f44b3a481d42ab294d.png" alt="ca549e6341642bdc7c9456f44b3a481d42ab294d.png" />
</p>
</div>


<p>
Note that now, we get linear extrapolation, because we are using a linear kernel. Note also that the hyperparameters do not mean anything in particular to us. They do not include the slope or intercept. We can work those out pretty easily though. The intercept is just a prediction at \(x=0\):
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">Kp</span> = sb**2 + sv**2 * (np.array([0]) - c)[:, <span style="color: #D0372D;">None</span>] * (X - c)[<span style="color: #D0372D;">None</span>, :]

Kp @ np.linalg.inv(K) @ y
</pre>
</div>

<pre class="example">
array([2.95418264])
</pre>

<p>
Not surprisingly, the intercept is about 3.0. We can similarly compute the slope as rise/run since we have a line in our predictions, and it is also approximately what we expect.
</p>

<div class="org-src-container">
<pre class="src src-ipython">(yp[-1] - yp[0]) / (Xp[-1] - Xp[0])
</pre>
</div>

<pre class="example">
2.022556668604536
</pre>
</div>

<div id="outline-container-org8864285" class="outline-4">
<h4 id="org8864285"><span class="section-number-4">3.1.1</span> Uncertainty quantification in GPR</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
One of the main reasons to use GPR is that you can estimate the uncertainty in predictions in a straightforward way. The covariance of a prediction is given by:
</p>

<p>
\(\mathbf{\sigma} = K(X*, X*) - K(X*, X) [K(X, X) + \sigma_n^2 \mathbf{I}]^-1 K(X, X*)\)
</p>

<p>
As we have done before, the square root of the diagonal is an estimate of the error in the prediction of each point.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">Xp</span> = np.linspace(0, 2)
<span style="color: #BA36A5;">yp</span> = Kp @ np.linalg.inv(K) @ y

<span style="color: #BA36A5;">K</span> = sb**2 + sv**2 * (X - c)[:, <span style="color: #D0372D;">None</span>] * (X - c)[<span style="color: #D0372D;">None</span>, :]
<span style="color: #BA36A5;">K</span> += 0.05 * np.eye(<span style="color: #006FE0;">len</span>(y))

<span style="color: #BA36A5;">Kp</span> = sb**2 + sv**2 * (Xp - c)[:, <span style="color: #D0372D;">None</span>] * (X - c)[<span style="color: #D0372D;">None</span>, :]
<span style="color: #BA36A5;">Kt</span> = sb**2 + sv**2 * (Xp - c)[:, <span style="color: #D0372D;">None</span>] * (Xp - c)[<span style="color: #D0372D;">None</span>, :]

<span style="color: #BA36A5;">sigma</span> = np.sqrt(np.diag(Kt - Kp @ np.linalg.inv(K) @ Kp.T))

plt.plot(X, y, <span style="color: #008000;">'b.'</span>)
plt.plot(Xp, yp)
plt.fill_between(Xp, yp + 2*sigma, yp - 2*sigma, alpha=0.2, color=<span style="color: #008000;">'gray'</span>)
plt.xlabel(<span style="color: #008000;">'x'</span>)
plt.ylabel(<span style="color: #008000;">'y'</span>)
plt.legend([<span style="color: #008000;">'data'</span>, <span style="color: #008000;">'GPR'</span>])
</pre>
</div>

<p>
ValueErrorTraceback (most recent call last)
&lt;ipython-input-18-675ed644b6aa&gt; in &lt;module&gt;
     11
     12 plt.plot(X, y, 'b.')
&#x2014;&gt; 13 plt.plot(Xp, yp)
     14 plt.fill_between(Xp, yp + 2*sigma, yp - 2*sigma, alpha=0.2, color='gray')
     15 plt.xlabel('x')
</p>

<p>
~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py in plot(scalex, scaley, data, *args, **kwargs)
   2794     return gca().plot(
   2795         *args, scalex=scalex, scaley=scaley, **({"data": data} if data
-&gt; 2796         is not None else {}), **kwargs)
   2797
   2798
</p>

<p>
~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in plot(self, scalex, scaley, data, *args, **kwargs)
   1663         """
   1664         kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D._alias_map)
-&gt; 1665         lines = [*self._get_lines(*args, data=data, **kwargs)]
   1666         for line in lines:
   1667             self.add_line(line)
</p>

<p>
~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py in __call__(self, *args, **kwargs)
    223                 this += args[0],
    224                 args = args[1:]
&#x2013;&gt; 225             yield from self._plot_args(this, kwargs)
    226
    227     def get_next_color(self):
</p>

<p>
~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py in _plot_args(self, tup, kwargs)
    389             x, y = index_of(tup[-1])
    390
&#x2013;&gt; 391         x, y = self._xy_from_xy(x, y)
    392
    393         if self.command == 'plot':
</p>

<p>
~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py in _xy_from_xy(self, x, y)
    268         if x.shape[0] != y.shape[0]:
    269             raise ValueError("x and y must have same first dimension, but "
&#x2013;&gt; 270                              "have shapes {} and {}".format(x.shape, y.shape))
    271         if x.ndim &gt; 2 or y.ndim &gt; 2:
    272             raise ValueError("x and y can be no greater than 2-D, but have "
</p>

<p>
ValueError: x and y must have same first dimension, but have shapes (50,) and (1,)
</p>

<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>



<div class="figure">
<p><img src="obipy-resources/ff89c57eaf339be5985eacf89c18aa90a6f59ccd/0ca0478e288cd29af9101c80e421405da73a3f86.png" alt="0ca0478e288cd29af9101c80e421405da73a3f86.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org51e0b20" class="outline-3">
<h3 id="org51e0b20"><span class="section-number-3">3.2</span> Combining kernels</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Here we consider modeling a slowly increasing periodic function.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">X</span> = np.linspace(0, 1, 40)
<span style="color: #BA36A5;">y</span> = 2 * X + 3 + np.sin(X * 20) + np.random.normal(0, 0.05, <span style="color: #006FE0;">len</span>(X))

plt.plot(X, y, <span style="color: #008000;">'b.'</span>)
</pre>
</div>

<pre class="example">
[&lt;matplotlib.lines.Line2D at 0x10167c99d0&gt;]
</pre>


<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>



<div class="figure">
<p><img src="obipy-resources/ff89c57eaf339be5985eacf89c18aa90a6f59ccd/a8567305b13793229dbe2fa639812e94c4db127e.png" alt="a8567305b13793229dbe2fa639812e94c4db127e.png" />
</p>
</div>

<p>
This looks like a sin wave superimposed on a line. A periodic kernel is defined as
</p>

<p>
\(k(x, x') = \sigma^2 \exp\left(-\frac{2 \sin^2(\pi|x - x'| / p)}{l^2}\right)\)
</p>

<p>
\(p\) is the periodicity and \(l\) is the lengthscale. A key feature of GPR is you can add two kernel functions together and get a new kernel. Here we combine the linear kernel with the periodic kernel to represent data that is periodic and which increases (or decreases) with time.
</p>

<p>
As before we use the log likeliehood to find the hyperparameters that best fit this data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">def</span> <span style="color: #006699;">LL</span>(params):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">sb</span>, <span style="color: #BA36A5;">sv</span>, <span style="color: #BA36A5;">c</span>, <span style="color: #BA36A5;">sp</span>, <span style="color: #BA36A5;">p</span>, <span style="color: #BA36A5;">l</span> = params
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">K1</span> = sb**2 + sv**2 * (X - c)[:, <span style="color: #D0372D;">None</span>] * (X - c)[<span style="color: #D0372D;">None</span>, :]
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">K2</span> = sp**2 * np.exp(-2 * np.sin(np.pi / p * np.<span style="color: #006FE0;">abs</span>((X[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])))**2 / l**2)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">K</span> = K1 + K2 + 0.05 * np.eye(<span style="color: #006FE0;">len</span>(y))
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> -1*(-0.5 * y @ np.linalg.inv(K) @ y - 0.5 * np.log(np.diag(K)).<span style="color: #006FE0;">sum</span>())

<span style="color: #BA36A5;">pars</span> = minimize(LL, [3, 2, 0, 1, 1, 0.01])
pars
</pre>
</div>

<pre class="example">
     fun: 21.273976491366337
hess_inv: array([[ 0.00206597,  0.0001018 , -0.00061243, -0.00687782,  0.00089709,
       -0.00092826],
      [ 0.0001018 ,  0.00342047, -0.00266409, -0.00363404,  0.00229339,
       -0.00109994],
      [-0.00061243, -0.00266409,  0.00369719,  0.00958791, -0.00267934,
        0.00050159],
      [-0.00687782, -0.00363404,  0.00958791,  0.0466703 , -0.00693871,
        0.0031568 ],
      [ 0.00089709,  0.00229339, -0.00267934, -0.00693871,  0.00270228,
       -0.00074091],
      [-0.00092826, -0.00109994,  0.00050159,  0.0031568 , -0.00074091,
        0.00126629]])
     jac: array([5.00679016e-06, 7.86781311e-06, 8.82148743e-06, 5.24520874e-06,
      6.43730164e-05, 1.07288361e-05])
 message: 'Desired error not necessarily achieved due to precision loss.'
    nfev: 1194
     nit: 47
    njev: 148
  status: 2
 success: False
       x: array([-2.47451474e-07,  3.89663806e-01, -1.48556646e+00,  6.48536273e-01,
       9.34604979e-01, -3.36433225e-01])
</pre>

<p>
And we check how the fit looks, and how it extrapolates.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">xp</span> = np.linspace(-1, 2, 200)

<span style="color: #BA36A5;">sb</span>, <span style="color: #BA36A5;">sv</span>, <span style="color: #BA36A5;">c</span>, <span style="color: #BA36A5;">sp</span>, <span style="color: #BA36A5;">p</span>, <span style="color: #BA36A5;">l</span> = pars.x

<span style="color: #BA36A5;">K1</span> = sb**2 + sv**2 * (X - c)[:, <span style="color: #D0372D;">None</span>] * (X - c)[<span style="color: #D0372D;">None</span>, :]
<span style="color: #BA36A5;">K2</span> = sp**2 * np.exp(-2 * np.sin(np.pi / p * np.<span style="color: #006FE0;">abs</span>((X[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])))**2 / l**2)
<span style="color: #BA36A5;">K</span> = K1 + K2 + 0.05 * np.eye(<span style="color: #006FE0;">len</span>(y))

<span style="color: #BA36A5;">Kp1</span> = sb**2 + sv**2 * (xp - c)[:, <span style="color: #D0372D;">None</span>] * (X - c)[<span style="color: #D0372D;">None</span>, :]
<span style="color: #BA36A5;">Kp2</span> = sp**2 * np.exp(-2 * np.sin(np.pi / p * np.<span style="color: #006FE0;">abs</span>((xp[:, <span style="color: #D0372D;">None</span>] - X[<span style="color: #D0372D;">None</span>, :])))**2 / l**2)
<span style="color: #BA36A5;">Kp</span> = Kp1 + Kp2


<span style="color: #BA36A5;">yp</span> = Kp @ np.linalg.inv(K) @ y
plt.plot(X, y, <span style="color: #008000;">'b.'</span>, xp, yp)
</pre>
</div>

<pre class="example">
[&lt;matplotlib.lines.Line2D at 0x101683f3d0&gt;,
 &lt;matplotlib.lines.Line2D at 0x10167c9b90&gt;]
</pre>


<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>



<div class="figure">
<p><img src="obipy-resources/ff89c57eaf339be5985eacf89c18aa90a6f59ccd/f869f7ca8770b59632567631ac678fc04e3c3a31.png" alt="f869f7ca8770b59632567631ac678fc04e3c3a31.png" />
</p>
</div>



<p>
Note that we get oscillatory + linear extrapolation behavior!
</p>
</div>
</div>
</div>


<div id="outline-container-orgb709d7e" class="outline-2">
<h2 id="orgb709d7e"><span class="section-number-2">4</span> Brief comparison of GPR and NN</h2>
<div class="outline-text-2" id="text-4">
<p>
GPR is called a non-parametric regression method. That is only partly true, there are hyperparameters that must be chosen in the kernels. In contrast, neural networks (and other physical models are called <i>parametric</i> models.
</p>

<p>
A key feature of GPR compared to other methods is that uncertainty estimates are a "built-in" feature, compared to parametric models where you might consider it an add-on feature that approximates the uncertainty. Although we say uncertainty analysis is built into to GPR, it also relies on some assumptions, e.g. that there is Gaussian noise in the data, and that the residual errors are Gaussian. If those are not true, then the uncertainty in a GPR is also an estimate.
</p>

<p>
For very large datasets GPR has a distinct disadvantage over neural networks. For \(n\) data points covariance matrix is an \(n \times n\), and we need the inverse of this array. Inverse calculations usually scale as \(O(n^3)\) so this can get expensive fast. Even after that, however, you have to do several matrix multiplications, including an \(m \times n\) covariance array, a \(n \times n\) inverse covariance array and the \(n \times 1\) array of known values. If is possible to compute one of these one time only, but for every prediction, one must compute the \(m \times n\) covariance array every time.
</p>

<p>
In contrast, for neural networks, all the time is spent upfront on training. After that, all the arrays of weights are fixed, and the computational time for predictions is constant (and usually comparatively small).
</p>
</div>
</div>


<div id="outline-container-orgb9d0b40" class="outline-2">
<h2 id="orgb9d0b40"><span class="section-number-2">5</span> GPR libraries</h2>
<div class="outline-text-2" id="text-5">
<p>
In this lecture we have examined GPR in a hand's on, practical and manual way. In practice, it is rare to do this anymore as there are libraries that automate much of the calculations. Using these requires a sophisticated understanding of how GP works though, and they are not easy to start with.
</p>

<dl class="org-dl">
<dt>scikit-learn</dt><dd><a href="https://scikit-learn.org/stable/modules/gaussian_process.html">https://scikit-learn.org/stable/modules/gaussian_process.html</a></dd>
<dt>Gpy</dt><dd><a href="https://sheffieldml.github.io/GPy/">https://sheffieldml.github.io/GPy/</a> (pytorch)</dd>
<dt>GPFlow</dt><dd><a href="https://gpflow.readthedocs.io/en/latest/intro.html">https://gpflow.readthedocs.io/en/latest/intro.html</a> (Tensorflow)</dd>
</dl>
</div>
</div>

<div id="outline-container-orgb14a901" class="outline-2">
<h2 id="orgb14a901"><span class="section-number-2">6</span> Summary</h2>
<div class="outline-text-2" id="text-6">
<p>
This lecture introduced GPR in a practical, by example way. There are formal ways to derive the equations we introduced, but they rely on a deep understanding of statistics that is beyond the scope of this class. These approaches provide a variety of insights to understand why GPR works, how it is related to other types of machine learning, etc.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: John Kitchin</p>
<p class="date">Created: 2020-03-31 Tue 07:32</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
