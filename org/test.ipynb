{"cells":[{"cell_type":"markdown","metadata":{},"source":["-   KEYWORDS: autograd\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Derivatives in scientific programming\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Derivatives play an important role in modeling engineering processes.\nThey serve mathematical roles in optimization where we need them to find\nstationary points (i.e. where the first derivatives are zero), and to\ndetermine if these points are minima, maxima or saddle points.\n\nDerivatives also play a central role in uncertainty propagation and\nsensitivity analysis. These analyses require derivatives of equations\nwith respect to parameters.\n\nDerivatives also serve in physical roles. When we write mass/energy\nbalances we are defining how those variables change in time, which is a\nderivative. If you recall Fick's law, we way that the flux of a material\nis proportional to the *gradient* in concentration, which is a\nderivative. In thermodynamics, we relate many properties to derivatives\nof some thermodynamic variable. For example, the heat capacity is\ndefined by a partial derivative of the enthalpy:\n$\\left(\\frac{\\partial H}{\\partial T}\\right)_P = C_p$. There are many\nmore examples where derivatives are important.\n\nWe usually think about deriving derivatives using calculus. That\nrequires, however, that you have an analytical equation, that you know\nhow to derive the derivative, and finally that you correctly evaluate\nthe result. When you have an analytical equation, that approach is\nprobably the best one when done correctly.\n\nIn many cases, however, we may not have an equation, or the equation\ncould change regularly or be tedious to derive the derivative. As we\nincreasingly express equations in the form of a program, it is\nincreasingly inconvenient and difficult to work through the program to\nderive derivatives. In these cases, we need a computational approach to\ngetting derivatives.\n\nWe have primarily considered two approaches to *estimating* or\n*approximating* derivatives so far:\n\n1.  `numpy.gradient`\n2.  `scipy.misc.derivative`\n\nBoth of these approaches have limitations we review below.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### ~numpy.gradient~\n\n"]},{"cell_type":"markdown","metadata":{},"source":["`numpy.gradient` uses\n[finite\ndifference](https://en.wikipedia.org/wiki/Numerical_differentiation) formulas to estimate the derivatives *from data*. This data\nmay be obtained from experiments, or by numeric integration of an ODE,\nor from the solution to a BVP. In these cases we do not have analytical\nformulas to get derivatives from, and we have to resort to numerical\nmethods.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n  %matplotlib inline\n  import matplotlib.pyplot as plt\n\n  ?np.gradient"]},{"cell_type":"markdown","metadata":{},"source":["The accuracy of these derivatives depends on the spacing between the\ndata points. We have seen the derivatives at the edges of the data are\nless accurate because a first-order equation is used by default.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["x = np.linspace(0.1, 1, 10)\n  y = x**0.5\n\n  plt.plot(x, 0.5 * x**-0.5, x, np.gradient(y, x, edge_order=2), 'r.')\n  plt.legend(['analytical', 'numeric'])"]},{"cell_type":"markdown","metadata":{},"source":["    <matplotlib.legend.Legend at 0x114cd1c18>\n\n![img](bd3cf4973c3e0c5bc37c7144ac8269f42e2bc862.png)\n\nYou may recall we can fit a polynomial to this data, and then easily get\nthe derivative of the polynomial. By increasing the polynomial order we\ncan improve the derivative estimates to a point. If you start\noverfitting, you will introduce wiggles into the data.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["p = np.polyfit(x, y, 5)\n  dp  = np.polyder(p)\n\n  plt.plot(x, 0.5 * x **-0.5, x, np.polyval(dp, x), 'r.')\n  plt.legend(['analytical', 'numeric'])"]},{"cell_type":"markdown","metadata":{},"source":["    <matplotlib.legend.Legend at 0x115148cf8>\n\n![img](5f423c75de6e3eb822a702fa2d829ecb15bf4134.png)\n\nLet's briefly review some linear algebra and the connection with\nderivatives.\n\nA central difference formula is:\n\n$y'(x_i) \\approx \\frac{y_{i+1} - y_{i-1}}{2h}$\n\nWe cannot evaluate this for y0 or y-1. We need a simpler formula for\nthat:\n\nWe use a forward formula at the beginning:\n$y'(x_0) \\approx \\frac{y_1 - y_0}{h}$\n\nand a backward formula at the end:\n$y'(x_{-1} \\approx \\frac{y_{-1} - y_{-2}}{h}$\n\nWe can express these formulas in matrix algebra form:\n\n$\\mathbf{y'} = \\mathbf{D} \\mathbf{y}$\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["x, h = np.linspace(0.1, 1, 10, retstep=True)\n  y = x**0.5\n\n  D = np.zeros((len(x), len(x)))\n  D += np.diag(np.ones(len(x) - 1) / (2 * h), 1)  # diagonal above main\n  D += np.diag(-np.ones(len(x) - 1) / (2 * h), -1)  #  diagonal below the main\n  D[0, 0:2] = np.array([-1, 1]) / h  # forward formula for the first row\n  D[-1, -2:] = np.array([-1, 1]) / h # backward formula for the last row\n  D"]},{"cell_type":"markdown","metadata":{},"source":["    array([[-10.,  10.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n           [ -5.,   0.,   5.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n           [  0.,  -5.,   0.,   5.,   0.,   0.,   0.,   0.,   0.,   0.],\n           [  0.,   0.,  -5.,   0.,   5.,   0.,   0.,   0.,   0.,   0.],\n           [  0.,   0.,   0.,  -5.,   0.,   5.,   0.,   0.,   0.,   0.],\n           [  0.,   0.,   0.,   0.,  -5.,   0.,   5.,   0.,   0.,   0.],\n           [  0.,   0.,   0.,   0.,   0.,  -5.,   0.,   5.,   0.,   0.],\n           [  0.,   0.,   0.,   0.,   0.,   0.,  -5.,   0.,   5.,   0.],\n           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  -5.,   0.,   5.],\n           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., -10.,  10.]])\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["dydx = D @ y\n\n  np.allclose(dydx, np.gradient(y, x))"]},{"cell_type":"markdown","metadata":{},"source":["    True\n\nThere are more accurate formulas to use for these that use more data\npoints, but in these cases it is better to use `np.gradient` because it\nalready handles these.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### ~scipy.misc.derivative~\n\n"]},{"cell_type":"markdown","metadata":{},"source":["When we have equations in the form of *functions* rather than data, we\ncan leverage `scipy.misc.derivative`. This function also works by using\nfinite differences, and so it would suffer from the same limitations on\naccuracy as we saw above with data. Nevertheless, if you don't have a\nbetter approach, it might still be useful.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n  from scipy.misc import derivative\n  ?derivative"]},{"cell_type":"markdown","metadata":{},"source":["The most crucial step is choosing an appropriate value for dx. Note that\n`derivative` does not return a function; we have to *wrap* it in a\nfunction definition to use it like a function.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def f(x):\n      return x**0.5\n\n  def dfdx(x, dx):\n      return derivative(f, x, dx)\n\n  plt.plot(x, 0.5 * x **-0.5, x, dfdx(x, dx=0.005), 'r.')\n  plt.legend(['analytical', 'numeric'])"]},{"cell_type":"markdown","metadata":{},"source":["    <matplotlib.legend.Legend at 0x1169f4400>\n\n![img](7248c5a19a5fab71130f20444bc58d53557147e5.png)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["derivative(f, x, 0.005)  # this is vectorized, so the helper function is not critical here"]},{"cell_type":"markdown","metadata":{},"source":["    array([1.58163348, 1.11812136, 0.91290263, 0.79058486, 0.70711562,\n           0.64550283, 0.59761812, 0.55901972, 0.52704831, 0.50000156])\n\nWe can combine the ideas for data and functions with\n`scipy.interpolate.interp1d`. This is similar in spirit to using\npolyfit, but the polynomials are locally fit rather than globally fit\nthrough all the data points. As with polyfit, this can result in\nspurious wiggles being introduced, especially near data points where\nthere are big changes.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.interpolate import interp1d\n  ?interp1d"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["x, h = np.linspace(0.1, 1, 10, retstep=True)\n  y = x**0.5 + np.random.normal(0, 0.005, size=x.shape)\n\n  af = interp1d(x, y, kind='cubic', bounds_error=False, fill_value='extrapolate')\n\n  def dfadx(x, dx):\n      return derivative(af, x, dx)\n\n  plt.plot(x, 0.5 * x **-0.5, x, dfadx(x, dx=0.005), 'r.')\n  plt.legend(['analytical', 'numeric'])"]},{"cell_type":"markdown","metadata":{},"source":["    <matplotlib.legend.Legend at 0x1152ebe48>\n\n![img](01b4a3c7f71e81cf26b6b323ee84444ed2c22b63.png)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["plt.plot(x, 0.5 * x **-0.5, x, np.gradient(y, x), 'r.')"]},{"cell_type":"markdown","metadata":{},"source":["    [<matplotlib.lines.Line2D at 0x116fecef0>,\n     <matplotlib.lines.Line2D at 0x116ff9080>]\n\n![img](d1736127354ddc95b543e904cb64374d2a9e1328.png)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Limitations of numeric derivatives\n\n"]},{"cell_type":"markdown","metadata":{},"source":["There are several limitations of numeric derivatives. The biggest one is\nthat they are all *approximations* to the real derivative, and their\naccuracy depends on how small the spacing between the data points is. If\nthe spacing is too small, however, these methods can suffer from\nnumerical instabilities. These issues are exacerbated with higher order\nderivatives; derivatives tend to magnify errors in data.\n\nFitting models to the data leads to analytical models that can be\nanalytically differentiated. Here you have to be aware of the properties\nof the model, and its derivatives.\n\nThe methods above apply to scalar functions of a single variable. It is\nnot convenient to use them for multivariable functions.\n\nNumdifftools ([https://numdifftools.readthedocs.io/en/latest/](https://numdifftools.readthedocs.io/en/latest/)) is a\nmore advanced library for numerical differentiation that can do\nmultivariable functions, but it too can have numerical instabilities and\nneeds to be checked for convergence.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Symbolic differentiation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["[https://docs.sympy.org/latest/tutorial/calculus.html](https://docs.sympy.org/latest/tutorial/calculus.html)\n\nComputer algebra systems have increasingly been able to compute symbolic\nderivatives of expressions.\n[sympy](https://docs.sympy.org/latest/index.html) can do some\n[calculus](https://docs.sympy.org/latest/tutorial/calculus.html),\nincluding taking derivatives symbolically.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sympy import *\n  x = symbols('x')\n\n  df = diff(x**0.5, x)\n  print(df)\n  df.subs(x, 0.5)\n\n  X = np.linspace(0.1, 2)\n  plt.plot(X, 0.5 * X**-0.5, 'r-', X,  [df.subs(x, a) for a in X], 'b.')\n  plt.legend(['analytical', 'symbolic'])"]},{"cell_type":"markdown","metadata":{},"source":["    0.5*x**(-0.5)\n\n    <matplotlib.legend.Legend at 0x117ef2e10>\n\n![img](fe2b6c3f23f357236ec9ada8ef285341a094017c.png)\n\nFor some applications, this is very useful. Symbolic derivatives do not\nwork on programs though, and in some cases there are not simple\nderivatives to find.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Automatic differentiation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The third kind of computational derivatives we need to know about is\ncalled\n[automatic\ndifferentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (AD). It is completely different from both finite\ndifferences and symbolic differentiation. In AD, we use the chain rule\nto take derivatives of computer programs.\n\nAD solves many of the problems described above:\n\n1.  It is not an approximation like the finite difference approach.\n2.  It works on programs, unlike symbolic differentiation\n\nHowever, these features come at some cost; we have to use an AD library\nand learn how to write code with it. Most importantly, AD is usually an\nadd-on feature and its implementation introduces some constraints on\nwhat can be programmed.\n\nThere are several AD frameworks available in Python that have been\ndeveloped for machine learning applications. The main ones in use today\nare:\n\n1.  autograd - [https://github.com/HIPS/autograd](https://github.com/HIPS/autograd)\n2.  Tensorflow - [https://www.tensorflow.org/](https://www.tensorflow.org/)\n3.  pytorch - [https://pytorch.org/](https://pytorch.org/)\n\nWe will focus on autograd for the rest of the semester.\n\nYou can install it like this:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["!pip install autograd"]},{"cell_type":"markdown","metadata":{},"source":["autograd works by modifying `numpy` so that derivatives can be\nautomatically computed.\n\n**The most important step** in using autograd is to import the autograd\nversion of numpy. Not doing this will lead to errors eventually.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import autograd.numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["#### Derivatives of scalar functions\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Autograd provides four basic derivative functions. We first consider the\nderivative of a scalar function, i.e. a function of several arguments\nthat outputs a number. There are two functions for this: `grad` and\n`elementwise_grad`.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from autograd import grad, elementwise_grad\n  ?grad"]},{"cell_type":"markdown","metadata":{},"source":["You use grad when your function outputs a single number, and you want a\nsingle derivative of that function with respect to an argument. For\nexample, it could be an objective function.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def f(x):\n      return x**2\n\n  dfdx = grad(f)\n\n  dfdx(5.0) # the analytical derivative is 2x"]},{"cell_type":"markdown","metadata":{},"source":["    10.0\n\nNote: we cannot use integers as the input. Why? Integers are *not\ndifferentiable*.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["dfdx(1.0)"]},{"cell_type":"markdown","metadata":{},"source":["    2.0\n\nWe have to be careful about what kind of argument we use. You should not\nuse lists when you mean arrays. Autograd can only work on arrays defined\nin the autograd.numpy library.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["dfdx([1.5, 2.0])"]},{"cell_type":"markdown","metadata":{},"source":["    ---------------------------------------------------------------------------\n    TypeError                                 Traceback (most recent call last)\n    <ipython-input-34-455fcca68474> in <module>()\n    ----> 1 dfdx([1.5, 2.0])\n    \n    ~/anaconda/lib/python3.6/site-packages/autograd/wrap_util.py in nary_f(*args, **kwargs)\n         18             else:\n         19                 x = tuple(args[i] for i in argnum)\n    ---> 20             return unary_operator(unary_f, x, *nary_op_args, **nary_op_kwargs)\n         21         return nary_f\n         22     return nary_operator\n    \n    ~/anaconda/lib/python3.6/site-packages/autograd/differential_operators.py in grad(fun, x)\n         23     arguments as `fun`, but returns the gradient instead. The function `fun`\n         24     should be scalar-valued. The gradient has the same type as the argument.\"\"\"\n    ---> 25     vjp, ans = _make_vjp(fun, x)\n         26     if not vspace(ans).size == 1:\n         27         raise TypeError(\"Grad only applies to real scalar-output functions. \"\n    \n    ~/anaconda/lib/python3.6/site-packages/autograd/core.py in make_vjp(fun, x)\n          8 def make_vjp(fun, x):\n          9     start_node = VJPNode.new_root()\n    ---> 10     end_value, end_node =  trace(start_node, fun, x)\n         11     if end_node is None:\n         12         def vjp(g): return vspace(x).zeros()\n    \n    ~/anaconda/lib/python3.6/site-packages/autograd/tracer.py in trace(start_node, fun, x)\n          8     with trace_stack.new_trace() as t:\n          9         start_box = new_box(x, t, start_node)\n    ---> 10         end_box = fun(start_box)\n         11         if isbox(end_box) and end_box._trace == start_box._trace:\n         12             return end_box._value, end_box._node\n    \n    ~/anaconda/lib/python3.6/site-packages/autograd/wrap_util.py in unary_f(x)\n         13                 else:\n         14                     subargs = subvals(args, zip(argnum, x))\n    ---> 15                 return fun(*subargs, **kwargs)\n         16             if isinstance(argnum, int):\n         17                 x = args[argnum]\n    \n    <ipython-input-31-acbbf0022719> in f(x)\n          1 def f(x):\n    ----> 2     return x**2\n          3 \n          4 dfdx = grad(f)\n          5 \n    \n    TypeError: unsupported operand type(s) for ** or pow(): 'SequenceBox' and 'int'\n\nHere is another error:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["dfdx(np.array([1.5, 2.0]))"]},{"cell_type":"markdown","metadata":{},"source":["    ---------------------------------------------------------------------------\n    TypeError                                 Traceback (most recent call last)\n    <ipython-input-35-7d335e2ead81> in <module>()\n    ----> 1 dfdx(np.array([1.5, 2.0]))\n    \n    ~/anaconda/lib/python3.6/site-packages/autograd/wrap_util.py in nary_f(*args, **kwargs)\n         18             else:\n         19                 x = tuple(args[i] for i in argnum)\n    ---> 20             return unary_operator(unary_f, x, *nary_op_args, **nary_op_kwargs)\n         21         return nary_f\n         22     return nary_operator\n    \n    ~/anaconda/lib/python3.6/site-packages/autograd/differential_operators.py in grad(fun, x)\n         25     vjp, ans = _make_vjp(fun, x)\n         26     if not vspace(ans).size == 1:\n    ---> 27         raise TypeError(\"Grad only applies to real scalar-output functions. \"\n         28                         \"Try jacobian, elementwise_grad or holomorphic_grad.\")\n         29     return vjp(vspace(ans).ones())\n    \n    TypeError: Grad only applies to real scalar-output functions. Try jacobian, elementwise_grad or holomorphic_grad.\n\nThe problem is with an array input, `f` is not a scalar function; it\noutputs an array.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["f(np.array([1.5, 2.0]))"]},{"cell_type":"markdown","metadata":{},"source":["    array([2.25, 4.  ])\n\nTo address this, we can use `elementwise_grad`.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from autograd import elementwise_grad\n  ?elementwise_grad"]},{"cell_type":"markdown","metadata":{},"source":["You use elementwise\\_grad when you might use an array as input, and you\nget an array of values out, but you want the derivative of each element\nin the output with respect to the corresponding element in the input.\nThis is still a *scalar* function in the sense that each element in the\ninput produces one element in the output.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["elementwise_grad(f)(np.array([1.5, 2.0, -2.0, 5.0]))\n\n  eg = elementwise_grad(f)\n  eg(np.array([1.5, 2.0, -2.0, 5.0]))"]},{"cell_type":"markdown","metadata":{},"source":["    array([ 3.,  4., -4., 10.])\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["[dfdx(_x) for _x in [1.5, 2.0, -2.0, 5.0]]  # equivalent to elementwise_Grad"]},{"cell_type":"markdown","metadata":{},"source":["    [3.0, 4.0, -4.0, 10.0]\n\nHere is an example usage similar to the examples we have used so far.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def f(x):\n      return x**0.5\n\n  df = elementwise_grad(f)  # This returns a callable function\n\n  x = np.linspace(0.1, 2)\n\n  plt.plot(x, 0.5 * x**-0.5, 'r-', x, df(x), 'b.')\n  plt.legend(['analytical', 'autograd'])\n  np.allclose(0.5 * x**-0.5, df(x))"]},{"cell_type":"markdown","metadata":{},"source":["    True\n\n![img](5c7b4b598f0651d5e02cc5d24239bc26862db0c6.png)\n\nThe AD derivatives are identical within tolerance to the analytical\nformula because autograd simply applies the chain rule to the program to\nevaluate the derivatives.\n\n**Limitation** Derivatives with integers is not well-defined since\nintegers are not continuous.\n\nIt might not seem like a big deal that this works. The significance\nreally shows when you have more complex programs. This Rube-Goldberg\nprogram is equivalent to the previous program. You could work out the\nderivative by the chain rule your self, but autograd has no problem\ndoing this through all the operations and loops!\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def f(x):\n      a = 2.0 * x\n      b = a**2\n      c = b / 4.0\n      d = c**0.5\n      for i in range(5):\n          d = d * 2\n\n      for i in range(5):\n          d = d / 2\n\n      e = np.sqrt(d)\n      return e\n\n  df = elementwise_grad(f)  # This returns a callable function\n\n  x = np.linspace(0.1, 2)\n\n  plt.plot(x, 0.5 * x**-0.5, 'r-', x, df(x), 'b.')\n  plt.legend(['analytical', 'autograd'])\n  np.allclose(0.5 * x**-0.5, df(x))"]},{"cell_type":"markdown","metadata":{},"source":["    True\n\n![img](5c7b4b598f0651d5e02cc5d24239bc26862db0c6.png)\n\nOf course, autograd cannot make derivatives where they are not defined.\nThe derivative of the square root function is not defined at $x=0$, and\nwe get warnings and a `nan` result if we try to evaluate it there.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df(0.0)"]},{"cell_type":"markdown","metadata":{},"source":["    /Users/jkitchin/anaconda/lib/python3.6/site-packages/autograd/numpy/numpy_vjps.py:99: RuntimeWarning: divide by zero encountered in double_scalars\n      defvjp(anp.sqrt,    lambda ans, x : lambda g: g * 0.5 * x**-0.5)\n    /Users/jkitchin/anaconda/lib/python3.6/site-packages/autograd/numpy/numpy_vjps.py:59: RuntimeWarning: divide by zero encountered in double_scalars\n      lambda ans, x, y : unbroadcast_f(x, lambda g: g * y * x ** anp.where(y, y - 1, 1.)),\n    /Users/jkitchin/anaconda/lib/python3.6/site-packages/autograd/numpy/numpy_vjps.py:59: RuntimeWarning: invalid value encountered in double_scalars\n      lambda ans, x, y : unbroadcast_f(x, lambda g: g * y * x ** anp.where(y, y - 1, 1.)),\n\n    nan\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Derivatives of multivalue functions - Jacobian\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Autograd really starts to shine when we have vector functions. If we\nhave a function that takes an input with $n$ and produces $m$ outputs,\nthen we frequently need to compute the derivatives of the output with\nrespect to the inputs. These are defined by:\n\n$\\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j}$\n\nautograd provides the `jacobian` function for this. Let's consider an\nexample:\n\n$f_1(x, y) = x^2 y$\n\n$f_2(x, y) = 5 x + \\sin(y)$\n\nThe Jacobian of this system is:\n\n\\begin{equation} \\left[\\begin{array}{cc} 2 x y & x^2 \\ 5 & \\cos y \\\n\\end{array}\\right] \\end{equation}\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from autograd import jacobian\n\n  def f(X):\n      x, y = X\n      return np.array([x**2 * y, 5 * x + np.sin(y)])\n\n  Jf = jacobian(f)\n\n  # now show the equivalence\n  x, y = 0.5, 0.5\n  print(Jf(np.array([x, y])))\n  print(np.array([[2 * x * y, x**2], [5, np.cos(y)]]))"]},{"cell_type":"markdown","metadata":{},"source":["    [[0.5        0.25      ]\n     [5.         0.87758256]]\n    [[0.5        0.25      ]\n     [5.         0.87758256]]\n\n**Limitation** Note the explicit use of arrays in the above code. Autograd\nrequires you to use arrays explicitly most of the time, and you can get\nerrors if you are lazy and use lists/tuples.\n\nWe use Jacobians in a variety of applications, but one important one is\nfor changing variables in integrations, presumably because this results\nin a simpler integral.\n\n$\\int \\int_R f(x, y) dx dy = \\int \\int_{R'} f(x(u, v), y(u, v)) \\left|\\frac{\\partial(x, y)}{\\partial(u, v)}\\right| du dv$\n\nWhere $\\left|\\frac{\\partial(x, y)}{\\partial(u, v)}\\right|$ is defined as\nthe determinant of the Jacobian:\n\n$\\left|\\begin{array}{cc} \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v} \\end{array}\\right|$\n\nHere is an example we work out that is adapted from:\n[http://www.stat.rice.edu/~dobelman/notes_papers/math/Jacobian.pdf](http://www.stat.rice.edu/~dobelman/notes_papers/math/Jacobian.pdf)\n\n![img](./screenshots/date-13-11-2019-time-12-45-10.png)\n\nExecuting that double integral in cartesian coordinates is not\nconvenient because the integral limits would be a function for $y$. If\nwe instead switch to polar coordinates, then we have the simpler limits\nof $\\rho$ from 0 to $r$, and $\\theta$ from 0 to $2\\pi$. There is no\n$f(x, y)$ here, the integrand is just 1.\n\nThis is a double integral, and we use `scipy.integrate.dblquad`.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.integrate import dblquad\n  ?dblquad"]},{"cell_type":"markdown","metadata":{},"source":["Return the double (definite) integral of \\`\\`func(y, x)\\`\\` from \\`\\`x =\na..b\\`\\` and \\`\\`y = gfun(x)..hfun(x)\\`\\`.\n\nWe want:\n\n$\\int_{\\rho=0}^{\\rho=1} \\int_{\\theta=0}^{\\theta=2\\pi} det(J) d\\rho d\\theta = \\pi$\n\nThat leads to this implementation:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def f(P):\n      rho, theta = P\n      return np.array([rho * np.cos(theta), rho * np.sin(theta)])\n\n  jf = jacobian(f)\n\n  def integrand(rho, theta):\n      J = jf(np.array([rho, theta]))\n      return np.linalg.det(J)\n\n  # integrand(y, x)\n  xa, xb = 0, 2 * np.pi\n  ya, yb = 0, 1\n\n  dblquad(integrand, xa, xb, ya, yb)"]},{"cell_type":"markdown","metadata":{},"source":["    (3.141592653589793, 3.487868498008632e-14)\n\nAnd the expected answer. Compare that to the cartesian coordinate\nsystem:\n\n$\\int_{-1}^1 \\int_{-\\sqrt{1 - x^2}}^{\\sqrt{1 - x^2}} dx dy$\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def integrand(y, x):\n      return 1\n\n  def yl(x):\n      return -np.sqrt(1 - x**2)\n\n  def yu(x):\n      return np.sqrt(1 - x**2)\n\n  dblquad(integrand, -1, 1, yl, yu)"]},{"cell_type":"markdown","metadata":{},"source":["    (3.1415926535897967, 2.000470900043183e-09)\n\nThe answer is the same, but the integral limits are more complex. Of\ncourse, one can invoke Kitchin's conservation of complexity law here; we\ncan give up the complexity of the limits if we take on the complexity of\nautograd.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Hessians\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix) is\na square matrix of second-order partial derivatives of a scalar-valued\nfunction.\n\n$\\mathbf{H}_{ij} = \\frac{\\partial^2 f}{\\partial x_i x_j}$\n\n`autograd.hessian` also returns a callable function.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from autograd import hessian\n\n  def f(X):\n      x, y = X\n      return x**2 + y**2\n\n  H = hessian(f)\n\n  H(np.array([0.5, 0.5]))"]},{"cell_type":"markdown","metadata":{},"source":["    array([[2., 0.],\n           [0., 2.]])\n\nThe Hessian is used to classify what kind of stationary points have been\nfound. It is also used in some optimization algorithms.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Applications to optimization\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.optimize import minimize\n  ?minimize"]},{"cell_type":"markdown","metadata":{},"source":["    # Out[1]:\n\nWe will consider the\n[Rosenbrock\nfunction](https://en.wikipedia.org/wiki/Rosenbrock_function), which has a minimum at (1, 1) with a value of 0. The\nstandard optimization approach is shown here for comparison.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.optimize import minimize\n\n  def rosenbrock(X):\n      x, y = X\n      return (1 - x)**2 + 100 * (y - x**2)**2\n\n  sol = minimize(rosenbrock, [1, 0])\n  print(sol)"]},{"cell_type":"markdown","metadata":{},"source":["         fun: 2.110919359934138e-11\n    hess_inv: array([[0.49386353, 0.98772557],\n          [0.98772557, 1.98044813]])\n         jac: array([-2.89523431e-07,  2.86334956e-08])\n     message: 'Optimization terminated successfully.'\n        nfev: 148\n         nit: 32\n        njev: 37\n      status: 0\n     success: True\n           x: array([0.99999541, 0.9999908 ])\n\nThe solution is pretty good, but we can get a better answer if we\nprovide the Jacobian. Usually you are expected to derive and implement\nthis. We do it in one like with autograd.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from autograd import grad\n  df = grad(rosenbrock)\n\n  sol_j = minimize(rosenbrock, [1, 0], jac=df)\n  print(sol_j)"]},{"cell_type":"markdown","metadata":{},"source":["         fun: 1.929228346365973e-14\n    hess_inv: array([[0.49289915, 0.98579551],\n          [0.98579551, 1.97658546]])\n         jac: array([-2.94106502e-07,  8.15718604e-09])\n     message: 'Optimization terminated successfully.'\n        nfev: 37\n         nit: 32\n        njev: 37\n      status: 0\n     success: True\n           x: array([0.99999986, 0.99999972])\n\nNote that the function is closer to zero (although it was small to start\nwith).\n\nFinally, we get an even better answer if we also provide the Hessian,\nand use an algorithm that uses the Hessian (most of them do not).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from autograd import hessian\n  hf = hessian(rosenbrock)\n  minimize(rosenbrock, [1, 0], jac=df, hess=hf, method='dogleg')"]},{"cell_type":"markdown","metadata":{},"source":["        fun: 4.930380657631324e-30\n       hess: array([[ 802., -400.],\n          [-400.,  200.]])\n        jac: array([-8.8817842e-14,  4.4408921e-14])\n    message: 'Optimization terminated successfully.'\n       nfev: 2\n       nhev: 1\n        nit: 1\n       njev: 2\n     status: 0\n    success: True\n          x: array([1., 1.])\n\nNote we get an almost exact answer, with only two function evaluations!\n\nYou can see that the Hessian returned by this solver is identical to the\nHessian we would compute.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["hf(np.array([1.0, 1.0]))"]},{"cell_type":"markdown","metadata":{},"source":["    array([[ 802., -400.],\n           [-400.,  200.]])\n\nNote that in the example where we just provided the Jacobian that the\nHessian is approximated. You can see that here. It is pretty close, but\nnot exact.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["print(np.linalg.inv(sol_j.hess_inv))"]},{"cell_type":"markdown","metadata":{},"source":["    [[ 802.02588665 -399.99966532]\n     [-399.99966532  200.00039681]]\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Summary\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Today we reviewed computational approaches to taking derivatives. The\nstar of this lecture is automatic differentiation.\n\nAutograd is pretty good, but it has some limitations. You should review\n[these\nbest practices](https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#supported-and-unsupported-parts-of-numpyscipy). One of the reasons we reviewed the first two methods\nis that we need to be able to verify results sometimes, and those\nmethods are useful for that.\n\nNext time we will look at several applications of AD in calculus,\nscience and engineering. After that, we will return to nonlinear\nregression and conclude with an introduction to machine learning.\n\n"]}],"metadata":[["org"],null,null],"nbformat":4,"nbformat_minor":0}